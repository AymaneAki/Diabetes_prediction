{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Logistic Regression\n",
       "\n",
       "Logistic Regression is a classification algorithm used to predict binary outcomes (0 or 1). \n",
       "It models the probability that a given input \\( x \\) belongs to a particular class using the sigmoid function.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Model\n",
       "The logistic regression model is defined as:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle h_\\theta(x) = \\sigma(\\theta^T x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "where \\( \\sigma(z) \\) is the sigmoid function:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\sigma(z) = \\frac{1}{1 + e^{-z}}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This function ensures that the output is always between 0 and 1, making it interpretable as a probability.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Cost Function\n",
       "The cost function for logistic regression is the binary cross-entropy loss:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "where:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y^{(i)} \\text{ is the actual label (0 or 1)}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle h_\\theta(x^{(i)}) \\text{ is the predicted probability}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle m \\text{ is the number of training examples}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Gradient Descent Update Rule\n",
       "To minimize the cost function, we use gradient descent:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The gradient of the cost function is:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "where \\( \\alpha \\) is the learning rate.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, Math\n",
    "\n",
    "def print_md(text):\n",
    "    display(Markdown(text))\n",
    "\n",
    "def print_math(text):\n",
    "    display(Math(text))\n",
    "\n",
    "# Introduction\n",
    "print_md(r\"\"\"# Logistic Regression\n",
    "\n",
    "Logistic Regression is a classification algorithm used to predict binary outcomes (0 or 1). \n",
    "It models the probability that a given input \\( x \\) belongs to a particular class using the sigmoid function.\n",
    "\"\"\")\n",
    "\n",
    "# Logistic Model\n",
    "print_md(r\"\"\"## Model\n",
    "The logistic regression model is defined as:\n",
    "\"\"\")\n",
    "print_math(r\"h_\\theta(x) = \\sigma(\\theta^T x)\")\n",
    "\n",
    "print_md(r\"\"\"where \\( \\sigma(z) \\) is the sigmoid function:\n",
    "\"\"\")\n",
    "print_math(r\"\\sigma(z) = \\frac{1}{1 + e^{-z}}\")\n",
    "\n",
    "print_md(r\"\"\"This function ensures that the output is always between 0 and 1, making it interpretable as a probability.\n",
    "\"\"\")\n",
    "\n",
    "# Cost Function\n",
    "print_md(r\"\"\"## Cost Function\n",
    "The cost function for logistic regression is the binary cross-entropy loss:\n",
    "\"\"\")\n",
    "print_math(r\"J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\")\n",
    "\n",
    "print_md(r\"\"\"where:\n",
    "\"\"\")\n",
    "print_math(r\"y^{(i)} \\text{ is the actual label (0 or 1)}\")\n",
    "print_math(r\"h_\\theta(x^{(i)}) \\text{ is the predicted probability}\")\n",
    "print_math(r\"m \\text{ is the number of training examples}\")\n",
    "\n",
    "# Gradient Descent Update Rule\n",
    "print_md(r\"\"\"## Gradient Descent Update Rule\n",
    "To minimize the cost function, we use gradient descent:\n",
    "\"\"\")\n",
    "print_math(r\"\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\")\n",
    "\n",
    "print_md(r\"\"\"The gradient of the cost function is:\n",
    "\"\"\")\n",
    "print_math(r\"\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\")\n",
    "\n",
    "print_md(r\"\"\"where \\( \\alpha \\) is the learning rate.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
